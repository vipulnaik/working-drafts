# Longtermist funding is expected to increase a lot in the coming years

While making my
[updates](https://github.com/vipulnaik/donations/commits) to the
[donations list website](https://donations.vipulnaik.com/), I noticed
some things that suggest that funding directed in a broadly
longtermist direction will increase a lot in the next few years. The
potential increase suggests that talent and capacity constraints will
be more critical than funding constraints in the next few years. I'm
sure elites/insiders in the fund allocation space have already thought
about it, but I haven't seen much public discussion around these
points.

## Major donors

### Jaan Tallinn's philanthropy pledge

On his [philanthropy page](https://jaan.online/philanthropy/), Jaan
Tallinn pledges, for each of the five years from 2020 to 2024, to
donate, in "endpoint grants" (i.e., grants that have reached their
endpoint rather than being yet to be regranted), at *least* the larger
of $2 million and 20,000 times the minimum price of Ethereum (ETH) in
the year.

Here's a very crude table with final-ish 2020 data, tentative 2021
data (based on ETH price charts and grants announced via the Survival
and Flourishing Fund, and keeping in mind that some of the SFF grants
are not endpoint grants), and some crude estimates for 2022.

Year | Minimum USD price of ETH in the year | Minimum amount to donate (USD) | Estimate of amount actually donated (USD)
-- | -- | -- |--
2020 | 110.30 | 2,206,000 | 4,270,000
2021 | 730.99 | 14,619,800 | 15,000,000, to 20,000,000
2022 | est. 2,000 - 4,000 | est. 40,000,000 to 80,000,000 | ??

A few notes by year.

#### 2020

The data for 2020 is final(ish) and based on Tallinn's philanthropy page.

#### 2021

A note on 2021 and the range for it; we have the Survival and
Flourishing Fund (SFF)'s 2021 grant recommendations for both the first
and the second half of 2021:

* 2021 H1: $9,513,000 recommended
* 2021 H2: $8,858,000 recommended

The total across these halves is $18,371,000. However, there are
several caveats:

* Tallinn's actual grants may not match the SFF recommendations in
  amount or timing, and he may also make some additional grants than
  SFF. This was the case in 2020.

* Some of the grants recommended by SFF are to the EA Funds for
  regranting. These don't count as endpoint grants directly; the
  extent to which they convert to endpoint grants depends on how
  quickly the EA Funds regrant the funds. The EA Infrastructure Fund
  [says](https://forum.effectivealtruism.org/posts/pwnvva4AHfxYZmaGM/ea-infrastructure-fund-may-august-2021-grant-recommendations)
  they've functionally spent down the funds, but I don't think we have
  a similar confirmation from the Long-Term Future Fund yet.

#### 2022

Cryptocurrency prices can fluctuate quite a bit, so we really don't
know what will happen with Ethereum prices. However, ETH prices have
been above 2,000 USD for most of the time since April, and above 4,000
USD for a large part of the time since October. It seems reasonably
likely that ETH prices will continue to stay above 2,000 USD for all
of 2022.

Moreover, the pledge in terms of the minimum price in the year is
likely just a floor; it's more likely that Tallinn will care more
about the prices of ETH around the times he is making grants, which
are more spread out over the year. So even if there's some flash crash
in ETH prices, if most of the year ETH prices stay high enough it's
likely he'll donate a lot.

Looking at these estimates, then, we see that *significant* amounts of
money will probably have to be disbursed as *endpoint* grants in 2022,
more than double what we saw in 2021.


### FTX Foundation

The [FTX Foundation](https://ftx.com/intl/foundation) has already been
making some donations, but the donations so far seem to be mostly
driven by user donation matching of user choices, rather than
foundation-level allocation. However, a few things are suggestive that
the FTX Foundation will start becoming a bigger player and will focus
on longtermist areas:

* Scale of money: The foundation's page says that the total amount
  earmarked for charity is over $16 billion. Much of this is the
  wealth of Sam Bankman-Friend and his co-founders, who are probably
  somewhat like-minded.

* Focus on longtermism: In a [fireside
  chat](https://www.youtube.com/watch?v=--tV8U3BbJk), Sam
  Bankman-Fried said that it's very hard to refute the math that shows
  that the long-term matters a lot just as a result of sheer
  numbers. Also, the hiring of Nick Beckstead as foundation CEO in
  late 2021
  ([LinkedIn](https://www.linkedin.com/in/nick-beckstead-7aa54374/),
  [friends-restricted Facebook
  post](https://www.facebook.com/nbeckstead/posts/10118866028010570))
  suggests a longtermist focus, considering that Beckstead's
  [research](https://www.nickbeckstead.com/research) has been on the
  importance of shaping the long-term future, and the [donations he
  has
  influenced](https://donations.vipulnaik.com/influencer.php?influencer=Nick+Beckstead)
  have been in longtermist areas.

* Plans to accelerate giving: The hiring of Nick Beckstead as FTX
  Foundation CEO suggests that FTX Foundation does plan to scale up
  and accelerate its giving.

We don't know how long it will take to meaningfully scale up giving;
it's possible that there won't be much happening in 2022, as FTX
Foundation works on its initial capacity-building.

### Open Philanthropy

Open Philanthropy is a [major funder in the AI safety
space](https://donations.vipulnaik.com/donor.php?donor=Open+Philanthropy&cause_area_filter=AI+safety),
and has also funded work related to [biosecurity and pandemic
preparedness](https://donations.vipulnaik.com/donor.php?donor=Open+Philanthropy&cause_area_filter=Biosecurity+and+pandemic+preparedness)
and [other global catastrophic
risks](https://donations.vipulnaik.com/donor.php?donor=Open+Philanthropy&cause_area_filter=Global+catastrophic+risks).

Since Open Philanthropy often makes large multi-year grants, their
donation amounts by year can fluctuate rather wildly (for instance, in
2021, they made a [$38.92 million 3-year
grant](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-security-and-emerging-technology-general-support-august-2021)
to CSET, as well as [another $8 million
grant](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-security-and-emerging-technology-general-support);
together, these grants account for well over half of their AI safety spend
in 2021.

The most recent public update from Open Philanthropy around their
support of these causes in their [2020 progress / 2021 plans
post](https://www.openphilanthropy.org/blog/our-progress-2020-and-plans-2021):

> We have now completed and published a number of reports on the
> likelihood of transformative AI being developed within the next
> couple of decades, three of which are publicly available: Joseph
> Carlsmith’s report estimating how much computational power it takes
> to match the human brain, Ajeya Cotra’s draft report on AI
> timelines, and Tom Davidson’s report forecasting the development of
> artificial general intelligence using a “semi-informative priors”
> framework. We also published David Roodman’s report modeling
> historic and future economic growth, which may help inform future
> funding priorities.
>
> Our worldview investigations team is now working on:
>
> * More thoroughly assessing and writing up what sorts of risks
> transformative AI might pose and what that means for today’s
> priorities.
> * Updating our internal views of certain key values, such
> as the estimated economic value of a disability-adjusted life year
> (DALY) and the possible spillover benefits from economic growth,
> that inform what we have thus far referred to as our “near-termist”
> cause prioritization work.

A more recent post titled [Technical Updates to Our Global Health and
Wellbeing Cause Prioritization
Framework](https://www.openphilanthropy.org/blog/technical-updates-our-global-health-and-wellbeing-cause-prioritization-framework)
suggests that Open Philanthropy plans to significantly increase its
spend rate overall (spending about 9% of funds) and discusses
specifically the plans around allocating more to GiveWell top
charities (as part of the global health and wellbeing grantmaking, as
opposed to the longtermist grantmaking). A [further
post](https://www.openphilanthropy.org/blog/2021-allocation-givewell-top-charities-why-we-re-giving-more-going-forward)
gives GiveWell-recommended allocation targets of $300 million for 2021
and $500 million each for 2022 and 2023.

The implication of the increased spend for longtermist causes is not
spelled out in the blog posts; for instance, it's unclear if the 9%
spend target applies only to the human-centric portion of the global
health and wellbeing portion of Open Philanthropy's giving, or to all
of the giving. If it applies to all of the giving, then, since the raw
increase in giving to GiveWell top charities won't fully cover the 9%,
this suggests that other cause areas will also see increased funding
in the next few years. I posted a
[comment](https://www.openphilanthropy.org/blog/technical-updates-our-global-health-and-wellbeing-cause-prioritization-framework#comment-1105)
seeking clarification on this point.

### Other big crypto donors

We already saw that a lot of Jaan Tallinn's wealth is tied to
Ethereum, and the size of his mostly longtermist grantmaking is tied
to Ethereum prices over the next few years. Tallinn is not the only
person with huge wealth gains due to crypto; there are several others,
including [a big anonymous MIRI donor as well as Vitalik
Buterin](https://intelligence.org/2021/05/13/two-major-donations/). As
Zvi Mowshowitz [writes in a section titled "Too Much
Money" (TMM)](https://www.lesswrong.com/posts/kuDKtwwbsksAW4BG2/zvi-s-thoughts-on-the-survival-and-flourishing-fund-sff#Too_Much_Money):

> The core reason for [Too Much Money] is crypto. If you invested
> early in crypto, there’s a very good chance you have TMM. You see
> the signs of this all over the space, and it’s not a coincidence
> people are paying premium prices for bored apes. I didn’t think of
> it in time to get in early, but in hindsight, the moniker fits.
>
> Regardless of how you would score the community’s performance on
> crypto, enough people who are EA/rationalist adjacent enough did buy
> enough that they’re in position to give such causes quite a lot, and
> also that’s where many of our giant funders got their bankrolls. If
> anything, as a group we are now overinvested, even if you are
> bullish on the space.

## Allocators and influencers

### Survival and Flourishing Fund (SFF)

Back when it started in 2019, SFF was still granting out some money it
had inherited from BERI, but it now acts as a purely virtual fund,
running a [S-process](https://www.youtube.com/watch?v=jWivz6KidkI) to
help funders such as Jaan Tallinn, Jed McCaleb, and David Marble (for
The Casey and Family Foundation) allocate funds for longtermist causes
based on the inputs of many recommenders.

As Jaan Tallinn explains on his [philanthropy
page](https://jaan.online/philanthropy/), the bulk of his grantmaking
is based on SFF recommendations. Also, Tallinn accounts for the bulk
of SFF's grantmaking; for instance, in 2021 H2, SFF recommended
$8,858,000 in grants from Tallinn, but only $500,000 from The Casey
and Family Foundation and only $250,000 from Jed McCaleb.

For 2022, if we go with the conservative estimate for Tallinn of $40
million (based on a minimum Ethereum price of $2,000) and assume two
rounds of grantmaking like previous years, then the rounds have to
average an allocation of $20 million from Tallinn alone. How
well-equipped is SFF to handle this grantmaaking?

In a [lengthy blog
post](https://www.lesswrong.com/posts/kuDKtwwbsksAW4BG2/zvi-s-thoughts-on-the-survival-and-flourishing-fund-sff),
Zvi Mowshowitz, who participates as one of the recommenders in the
second half of 2021, talks about a problem of "Too Much Money" that,
in his view, SFF *already* experienced in the 2021 H2 round (where the
total amount being distributed was about $9.6 million, including
$8,858,000 from Tallinn, $500,000 from The Casey and Family
Foundation, and $250,000 from Jed McCaleb). Since this is half of what
we expect in each round in 2022, this suggests that the TMM problem
may become even more acute in 2022, unless other things change.

> Anyway, I mention all this because there are several senses in which
> the process and the system around it could be considered to have
> TMM, or Too Much Money.
>
> 1. EA has TMM.
> 2. SFF had TMM.
> 3. A lot of people in crypto have TMM.
> 4. SFF Grants that were too large might cause organizations to have TMM.
> [...]
> However, if one is limited, for whatever reason, to giving away
> money to charitable organizations that already exist and which
> legibly fit the mold of EA causes and frameworks, then in the
> context of funds earmarked for such things, EA does seem from where
> I sit to have TMM.
>
> In that context, SFF also had TMM. If you looked at each individual
> recommender’s allocation, everyone gave away substantially less than
> all the money. When I went looking for additional organizations to
> encourage to apply, I did find (or think of) one and with time
> likely could have found more, but my guess is that the organizations
> that didn’t apply despite being legible EA causes I’d have been
> excited to fund, did so because they didn’t need the funds.
> [...]
> These idiosyncratic large grants are, in my view, a very good thing,
> but when looking to allocate SFF funds it means there’s a lot more
> to distribute and less places available to distribute to.

### Long-Term Future Fund (LTFF)

The EA Funds generally aim to allocate money within a few months of
receiving it, so we may think of them more as "allocators" than
"funders"; the funds they are able to give out in 2022 and 2023 will
depend on the funds they raise *in those years*.

A [March 2021
post](https://forum.effectivealtruism.org/posts/nLxpFeEs6kAdgjRWz/the-long-term-future-fund-has-room-for-more-funding-right)
by Asya Bergal suggested optimism that the Long-Term Future Fund could
scale up its giving to the range of $4 million to $8 million in
2021. Subsequently the LTFF
[applied](https://forum.effectivealtruism.org/posts/nLxpFeEs6kAdgjRWz/the-long-term-future-fund-has-room-for-more-funding-right?commentId=9cBRWH9L6BDhd5TCF)
for a grant for 2021 H1 and [got
$675,000](https://survivalandflourishing.fund/sff-2021-h1-recommendations),
then applied again for 2021 H2 and [got
$1,417,000](https://survivalandflourishing.fund/sff-2021-h2-recommendations).

## Recipient organizations

### MIRI seems to have a strong baseline level of funding

MIRI has received several large crypto donations; most recently, they
received a [$15.6 million donation meant to be spent over the next
five
years](https://intelligence.org/2021/05/13/two-major-donations/). Although
this donation is already made and therefore doesn't count as future
funding increases, to the extent it already covers some of MIRI's
basic operating costs, it means that funds from other funders (such as
Tallinn, FTX Foundation, or Open Philanthropy) are free to chase other
opportunities.

### Larks' assessment suggests that for AI safety in particular, the best organizations are well-funded

In a [section of a review of Larks' 2020 AI Alignment
 Review](https://www.lesswrong.com/posts/uEo4Xhp7ziTKhR6jq/reflections-on-larks-2020-ai-alignment-literature-review#Scalable_uses_for_money)
 (original review by Larks
 [here](https://forum.effectivealtruism.org/posts/K7Z87me338BQT3Mcv/2020-ai-alignment-literature-review-and-charity-comparison)),
 Alex Flint writes:

> Larks notes that much of the best research is being conducted within
> large organizations that already have ample funding, and are neither
> accepting of nor in need of additional funding at this time. This is
> both heartening and distressing.
>
> It is heartening, of course, to see important research being funded
> at such a level that in at least several prominent cases further
> funding by individual donors is literally impossible, and in several
> additional cases seems to be explicitly un-sought after by the
> organizations themselves.
>
> But it is also a little distressing that after 20 years of work in
> AI alignment (counting from the date that MIRI, then the Singularity
> Institute for Artificial Intelligence, was founded), we have neither
> a resolution to the AI alignment problem nor any scheme for scalably
> utilizing funds to find one. What would a scalable scheme for
> resolving the AI alignment problem look like, exactly? Is depth
> scalable? If not, then why exactly is that?


## Capacity to allocate money

#### Survival and Flourishing Fund

