# SBF, extreme risk-taking, and effective altruism

The collapse of Sam Bankman-Fried (SBF) and his companies FTX and
Alameda Research is the topic du jour on the Effective Altruism Forum,
and there have been [several posts on the
Forum](https://forum.effectivealtruism.org/topics/future-fund?sortedBy=new)
discussing what happened and what we can learn from it. The extent to
which fraud was involved is not clear.

Based on information so far, fraud and deception happened and were
likely key to the way things played out and the extent of damage
caused. The trigger seems to be the [big loan that FTX provided to
Alameda Research to bail it
out](https://twitter.com/LucasNuzzi/status/1590122590206824448), using
customer funds for the purpose. If FTX hadn't bailed out Alameda, it's
quite likely that the spectacular death of FTX we saw (with depositors
losing all their money as well) wouldn't have happened. But it's also
plausible that without the loan, the situation with Alameda Research
was dire enough that Alameda Research, and then FTX, would have died
due to the lack of funds. Hopefully that would have been a more
graceful death with less pain to depositors. That is a very important
difference. Nonetheless, I suspect that by the time of the bailout, we
were already at a kind of endgame.

In this post, I try to step back a bit from the endgame, and even get
away from the specifics of FTX and Alameda Research (that I know very
little about) and in fact even get away from the specifics of SBF's
business practices (where again I know very little). Rather, I talk
about SBF's overall philosophy, *as he has articulated himself*, that
I think is very important to understanding the overall way things
played out. And I also discuss the relationship between the philosophy
and the ideas of effective altruism, both in the abstract and as
specifically championed by people such as William MacAskill.

Broadly, I make two claims:

* Claim 1: SBF engages in extreme risk-taking that is a crude
  approximation to the idea of expected value maximization as
  perceived by him.

* Claim 2: At least part of the motivation for SBF's risk-taking comes
  from ideas in effective altruism, and in particular specific points
  made by William MacAskill (and others). While personality probably
  accounts for a lot of SBF's decisions, the role of EA ideas as a
  catalyst cannot be dismissed based on the evidence.

## Claim 1 justification: SBF engages in extreme risk-taking

I won't really provide justification for Claim 1; I'll note in passing
that a lot of commentary both on the EA Forum (such as [Kerry
Vaughan's
summary](https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=GoDd83K7ipktDtWWs))
and in external press coverage (see for instance
[Axios](https://www.axios.com/2022/11/11/sam-bankman-fried-risk-taking-fail)). The
justification for Claim 2 also provides some "from the horse's mouth"
evidence for Claim 1.

## Claim 2 justification: At least part of the motivation for SBF's extreme risk-taking comes from effective altruist ideas

### SBF's articulation in a fireside chat

In a [fireside chat with Stanford EA, SBF gives advice to students
based on his own
experience](https://www.youtube.com/watch?v=--tV8U3BbJk&t=3107s). At
first listening, everything he says sounds quite reasonable (in
general, SBF's public persona feels very reasonable -- something that
falsely causes people to feel reassured!).

Here is the transcript from YouTube, lightly edited by me for
sentencification and removal of "um"s and uh"s; you can watch the
video or read the original transcript on YouTube by clicking "Show
transcript" in the options under "..." below the video. I have
highlighted the portions most relevant to my points, but have not
elided any other stuff within those segments of the video.

> Moderator (51:52): I think you basically answered this already but
> what concrete advice do you have for students for how they should be
> spending their time how to be more ambitious um how to better optimize
> for their goals figuring out what their goals might be or ought to be
> um maybe what would you do differently if you were a student or a
> first year now at MIT or Stanford? And yeah any last words that you'd
> want to leave the audience with before wrapping up.

> SBF (52:20): I's go to back to 2010 drop out and buy a lot of
> bitcoin but but but seriously i think there's something a little bit
> true there although that's not exactly what I think I could have
> predicted um which is that in 2012 um I had a friend at MIT who was
> sort of bored one day I think some some guy I don't remember who
> gave one free bitcoin to every MIT student around then i think there
> was like I don't know I was like well like five dollars at the time
> or something. Anyway, one of my friends, Gary, got bored and built
> some Bitcoin arbitrage bots for the nascent crypto exchanges that
> were around back in the early 2010s and made some money doing it;
> not a lot, he sort of saturated the market.  There wasn't a lot of
> volume but that was pretty cool. I never really checked it out that
> much. He was kind of tempted to got distracted he stopped doing this
> there's like it wasn't big enough field to make much and then
> neither of us thought about crypto again for five years. And then I
> called him up and we founded Alameda together. Certainly in
> retrospect it's hard to argue that like it wouldn't have been
> correct for us to just drop everything and do that back in 2012.

> SBF contd (53:45): And obviously there's a lot of stupid retroactive
> retrospective thinking there where like we couldn't have known what
> would happen but i actually think at the time we should have done it.
> We shouldn't have been able to predict how well it would have gone but
> diving into something that seems exciting and giving it your all and
> seeing how far it will go -- I think it's just like an incredibly good
> strategy in life and it's way better than you know sort of sticking
> around for another few years not doing much or just sort of like
> following the status quo. If you see a great opportunity I sort of
> think take it whatever it is. If it seems way better than whatever
> else you'd be doing by some sort of like **weird expected value
> calculation** that seems like it can't possibly be right but kind of
> feels cool i think it is **probably right in expectation** and yeah
> **it'll probably fail that's okay most things do** you try another
> thing. I don't know that that could be in a lot of different ways
> right like that could be some earning to give startup that could be
> jumping in some EA organization that could be taking charge of running
> Stanford EA that could be working you know diving into some biorisk
> research ir some other wacky thing like i don't know but there are a
> lot of awesome things to do out there and, you know, try them see how
> it goes! Try things that seem like they'll either be the right thing
> for you to do for the time being and teach you a lot or like the
> upside if they go extremely well is extremely high and like if the
> thing you're doing is neither of those, keep your eyes
> open for something else!

And earlier in the talk, SBF says:

> SBF (44:05): um i think there's been a lot of very very bad messaging
> over the years on that. I think there's a lot of messaging that is all
> a funding bottleneck and then a pretty sharp turn towards it's all a
> talent bottlenec. I think they're both wrong um my sense is that both
> matter. I think like as i've sort of spent more time trying to find
> things to fund. I found more things to fund and and don't currently
> feel like strongly under constrained on funding and over constrained
> on talent. I think both are very much limiting factors. And there are
> ways to really scale up the amount of of good that you can give to.
> So what are some ways to do it? First of all, I think it's sort of
> like a little awkward but it's just true um and probably not worth
> like you know trying to to sort of ignore that like on the funding
> side it it's probably going to be very top-heavy. It's a property of
> how the world works today that like the distribution of how much you
> can make over various things is not it's not like a normal
> distribution like the tail is way fatter. And it just has a pretty
> straightforward implication, I think, which is that like if earning to
> give is what you're thinking of doing and to be very clear I do think
> that can be incredibly valuable and i don't think that we are
> unconstrained on funding, **I think you should be thinking big**. I
> think **you should be thinking in expected value terms what's the
> thing you can do that will make the most [money]**. And I want to flag
> there that **if you think that the odds that you will achieve that
> target through the path are above 30 percent**, you're almost
> certainly not being ambitious enough! **It is almost certainly going
> to be the case that there is a risk-reward trade-off here that the
> things that make the most in expected value terms are things that will
> probably fail** and that if you're playing this correctly you should
> be it's very likely you should be pursuing a path where you think that
> **the median amount that you end up being able to donate is zero** or
> very close to it like it's sort of brutal and weird that's that's how
> the math works um i it not always but but I think more often than not
> that like um this is like super top heavy. You should be looking for
> things that have extremely high upside and willing to accept that they
> might fail, **willing to accept that they will probably fail** and to
> acknowledge that **we're trying to maximize our collective total
> impact and expected value on the world and you know there's no special
> virtue associated with having at least some impact like this stuff is
> linear**. Expected values: I think are pretty brutal but
> they are what they are! If your vision for what you're gonna do seems
> very likely to work you should think about how to make that vision
> more ambitious such you know obviously maximizing for how much it will
> work given that but like probably you're not being ambitious enough if
> it seems like it'll probably work. Although it should seem like it
> could plausibly work or otherwise probably it's a mistake.

### The expected value argument and its connection with effective altruism

It's folk wisdom that personal (selfish) utility for individuals tends
to be *less than linear* in the money they have; an idea that is also
widely known as the [diminishing marginal utility of
money](https://www.economicshelp.org/blog/12309/concepts/diminishing-marginal-utility-of-income-and-wealth/). One
common (though probably inaccurate) approximation is that utility to
individuals is approximately [logarithmic in
money](https://economics.stackexchange.com/questions/23952/is-the-utility-of-money-actually-logarithmic). These
general ideas are well-known in economics and among a lot of
intellectuals including many in the effective altruist movement.

The "altruistic" twist here is that for individuals interested in
altruistic impact, utility is much closer to being linear in money
than logarithmic. Or, we don't quite see diminishing marginal utility
of money for altruistic purposes, at least at the amounts of money
that most people can make. That's because *the problems of the world
are huge and can absorb huge amounts of money* (this is true for most
big problems, ranging from climate change to AI safety to global
health and development to animal welfare). So basically doubling your
wealth that you intend to allocate to charity should approximately
double your impact.

The basic idea is covered in a [post by Paul
Christiano](https://rationalaltruist.com/2013/02/28/risk-aversion-and-investment-for-altruists/)
(also [cited by 80,000
Hours](https://80000hours.org/2015/10/common-investing-mistakes-in-the-effective-altruism-community/))
but he's only looking at financial investments, in contrast with SBF
preaches and practices defining one's whole life / earning-to-give
trajectory around risky high-expected-value bets.

Will MacAskill, who has been an influence to SBF, has made these sorts
of points. The [video I could most readily find was a deep dive with
Ali Abdaal](https://youtu.be/YkdI8ztqWZc?t=10140) and was talking
about altruistic impact through "direct work" rather than donations,
but elsewhere in the video he does suggest a kind of exchange rate
between the two depending on one's direct impact in comparison to the
value of donations.

> Will MacAskill (2:49:00): This also is a difference between if you're
> trying to optimize for impact versus income so yeah you might think
> like okay got a couple of million in the bank now i'm just going to be
> happy with that like i can just seek that out like additional money's
> not worth that much more. Because you've got like it is three million
> YouTube subscribers?

> Ali Abdaal: About that

> Will MacAskill: Okay, yeah, so you're like if I had six million I'd
> have a bit more money but it's not going to be a huge difference in my
> well-being, [so] I'm not particularly motivated to grow the
> numbers.

> Ali Abdaal: except i don't have like an impact goal

> Will MacAskill: Exactly! But now if you're having impacts yeah how much better the six
> million subscribers than three million.

> Ali Abdaal: yeah way better

> Will MacAskill: Probably about twice as good like maybe not exactly
> like but like to first approximation yeah and so having altruistic
> impact in mind gives like much stronger arguments for scaling.

### Linearity on the low end: the lower bound of zero impact and non-consideration of *negative* impact by losing money

In the above discussion, my focus when talking about the
close-to-linear altruistic returns of money was on the upside/positive
side: you can scale up giving since the world's problems are so
big. However, there's another direction where this is important as
well: the direction toward zero (and beyond?).

One implicit idea in SBF's discourse, as seen in Stanford EA, is the
idea that utility is close enough to linear in money, and as an
important corollary, there's a lower bound at zero. The worst-case
outcome here is making nothing, in which case you make no donations
and therefore have effectively zero impact. So risk-taking has very
high upside but only a limited downside -- in the worst case, you're
wiped out, you declare bankruptcy, maybe you even die penniless.

From a *selfish* perspective, this is a pretty bad outcome (and
indeed, a logarithmic model of utility would give an infinite
*negative* utility to having no money). So from a selfish perspective,
there's a big downside to being wiped out, and this is part of what
motivates risk-aversion.

From an altruistic perspective, however, getting to zero money is a
bad outcome but only to the extent that it represents the absence of
good outcomes. So it's an outcome that you try to avoid, but not all
that desperately.

Moreover, this simple framework was developed mostly in connection
with people managing their own savings, rather than running complex
companies that manage *other people's* investments and assets. So it
doesn't even begin to grapple with the idea of going *negative* and
the utility implications of that. Of course, personal wealth can be
negative when one puts money on a credit card or takes a student loan,
but these are relatively small amounts and people generally start
thinking of altruism when they're no longer in significant debt. My
guess is that SBF (consciously or subconsciously) rounds up "going
negative" to zero because ultimately it just means he's able to donate
zero money.

### Startup risk and the kicking in of caution

A lot of what SBF said about risk-taking makes a lot of sense in the
context of somebody trying a startup idea (having earmarked some sort
of safety net that they won't touch, and then using other funds from
themselves or outside investors that are explicitly understood to be
for the purpose). What also tends to happen is that once the startup
starts succeeding and real people start depending on it for real
stuff, it starts moving in a more conservative direction -- reducing
the riskiness of its actions. There are probably four factors that
push in that direction:

1. **The founders/owners now have more to lose from a purely selfish
   perspective**; this essentially comes from the "diminishing marginal
   utility of money" idea albeit it may or may not be seen in purely
   financial terms. For instance, after a company grows from
   near-nothing to being worth a few million, and the founders have
   shares worth a decent chunk of that, they are at risk of losing
   that money if they tank the startup.

2. **The founders/owners have a desire to succeed and to not mess
   things up (e.g., because they now feel more passionately about the
   thing they're building, they feel attached to its success, or to
   avoid embarrassment)**. Messing up an already-big company feels
   more embarrassing, and can be more guilt-inducing as well to the
   extent that one sees the pain caused to others.

3. **The founders/owners have needed to involve other stakeholders,
   who also can lose out if things go bad**. This includes investors,
   employees, customers, partners, etc. Some of them may have
   incentives to take more risk (particularly investors who want to
   get big payouts from a diversified portfolio) but others benefit
   from greater stability and less risk. Moreover, since different
   stakeholders see the riskiness of various actions differently, and
   some level of agreement is needed, the overall direction will be
   toward less risk.

4. **Third parties may put more pressure of various sorts**; this
   includes regulators, hackers, a hostile press, or various other
   actors. In the face of this pressure, more caution and care may be
   needed.

A [great post by Dan Luu](https://danluu.com/wat/) talks about how
Google and Microsoft ultimately got serious about security after
embarrassing incidents. He writes:

> Google didn't go from adding z to the end of names to having the
> world's best security because someone gave a rousing speech or wrote
> a convincing essay. They did it after getting embarrassed a few
> times, which gave people who wanted to do things “right” the
> leverage to fix fundamental process issues. It's the same story at
> almost every company I know of that has good practices. Microsoft
> was a joke in the security world for years, until multiple
> disastrously bad exploits forced them to get serious about
> security. This makes it sound simple, but if you talk to people who
> were there at the time, the change was brutal. Despite a mandate
> from the top, there was vicious political pushback from people whose
> position was that the company got to where it was in 2003 without
> wasting time on practices like security. Why change what's worked?

So what was special about the SBF situation where they were able to
get to such a huge scale *without* these sorts of things kicking in?
Let's go through the four points:

1. **The founders/owners now have more to lose from a purely selfish
   perspective**: I think that although this was true, it probably
   wasn't as true in SBF's *perception* because his mental model was
   that of altruistic impact and linear utility. So making what he
   considered a positive-EV bet when the company was worth $5 billion
   may not have felt that different from making a positive-EV bet when
   the company was worth $5 million. So at least the absence of this
   particular mechanism was tied to the altruistic endgoal of the
   money.

2. **The founders/owners have a desire to succeed and to not mess
   things up (e.g., because they now feel more passionately about the
   thing they're building, they feel attached to its success, or to
   avoid embarrassment)**: My guess is that while SBF obviously had a
   desire to succeed and not mess things up, he didn't actually feel
   that passionate about the value of the work he was doing and saw it
   as a gamble to make money; as long as it was EV-positive, he was
   willing to take big risks even after amassing a lot of wealth.

3. **The founders/owners have needed to involve other stakeholders,
   who also can lose out if things go bad**: The failure of this
   mechanism doesn't seem directly tied to SBF's EA connection, but
   may be more of a feature of the business: they were able to get to
   a fairly large scale without having a lot of different
   stakeholders, and were also able to preserve a fair amount of
   secrecy despite the openness of the blockchain.

4. **Third parties may put more pressure of various sorts**: This
   didn't happen ... until it did, and then everything collapsed. The
   failure here in the wider world seems mostly unrelated to EA and
   may have more to do with the novelty of the space and therefore the
   lack of relevant critical expertise; however, the failure to notice
   this within EA was likely due to EA's positive impression of SBF
   and his expected value-maximizing ideals.
