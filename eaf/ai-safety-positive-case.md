As I understand it, there are two parts to the case for a focus on AI
safety research:

1. If we do achieve AGI and the AI safety / alignment problem isn't
   solved by then, it poses grave, even existential, risks to
   humanity. Given these grave risks, and some nontrivial probability
   of AGI in the medium-term, it makes sense to focus on AI safety.

2. If we are able to achieve a safe and aligned AGI, then many other
   problems will go away or at least get much better or simper to
   solve. So, focusing on other cause areas may not matter that much
   anyway if a safe/aligned AGI is likely in the near term.

I've seen a lot of fleshing out of 1; in recent times, it seems to be
the dominant reason for the focus on AI safety in effective altruist
circles, though 2 (perhaps without the focus on "safe") is a likely
motivation for many of those working on AI development.

The sentiment of 2 is echoed in many texts on superintelligence. For
instance, from the preface of Nick Bostrom's *Superintelligence*:

> In this book, I try to present the challenges presented by the
> prospect of superintelligence, and how we might best respond. This
> is quite possibly the most important and most daunting challenge
> humanity has ever faced. And -- whether we succeed or fail -- it is
> probably the last challenge we will ever face.

Similar sentiments are found in Bostrom's [Letter from
Utopia](https://www.nickbostrom.com/utopia.html).

Historical aside: MIRI's motivation around AI started off more around
2 and gradually moved to 1 -- an evolution that you can see in the
[timeline of
MIRI](https://timelines.issarice.com/wiki/Timeline_of_Machine_Intelligence_Research_Institute)
that I financed and partly wrote.

Another note: whereas 1 is a strong argument for AI safety even at low
but nontrivial probabilities of AGI, 2 becomes a strong argument only
at moderately high probabilities over a short time horizon. So if one
as a low probability estimate for AGI in the near-term, only 1 may be
a compelling argument even if both 1 and 2 are true.

So, question: what are some interesting analyses involving 2 and their
implications for the relative prioritization of AI safety and other
causes that safe, aligned AI might solve? A few example causes I'm
most interested in:

* Animal welfare: Would safe, aligned AGI put an end to the suffering
  of animals endured in factory farming and/or in the wild? Does this
  consideration meaningfully impact current prioritization of (and
  within) animal welfare? And does it cause anybody interested in
  animal welfare to focus more on AI safety?

* Life extension: Would safe, aligned AGI result in radical
  improvements to longevity? Does this consideration meaningfully
  impact current prioritization of (and within) life extension work?
  And does it cause anybody interested in life extension to focus more
  on AI safety?

* Global health: Would safe, aligned AGI solve global health issues?
  Does this consideration meaningfully impact current prioritization
  of (and within) global health? And does it cause anybody interested
  in life extension to focus more on AI safety?

Thanks to Issa Rice for the *Superintelligence* quote and many of the
other links!
