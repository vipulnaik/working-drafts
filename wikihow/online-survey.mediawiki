This guide covers the basics of online survey design, response collection, and analysis. It's not a how-to for the steps associated with any specific survey tool. Rather, it provides a general framework that will help select and use the best survey creation and survey distribution tools for your purpose.

== Steps ==

=== Identifying Your Goals for the Survey ===

# Think through the goals you want to accomplish with the survey. Some examples of goals are below. Note that these aren't totally separate from each other:
#* Getting actionable feedback. This is one of the typical goals of surveys of customers by businesses. The business wants to know what particular things the customer liked or disliked about the product or service, so that it can invest better in resources. Similarly, actionable feedback might be a goal of a teacher surveying students.
#* Getting information on who prefers what, from a constrained set of options. Surveys that involve single questions asking people to choose between options are called "polls". Polls may be used to drive decisions on what option to select (for instance, selecting a time or venue for a meeting) or as proxies for other, more official polls (such as pre-election polling, that is used to forecast the winner and victory margins in official elections).
#* Getting information on brand awareness, recognition, and loyalty.
#* Getting correlational information. For instance, are males more likely to be interested in a product than females? Do people who love watching sports games online also like endorsements by sports stars, compared to people who prefer to watch them on TV?
# Keep in mind all the stakeholders involved, to whom you will show the survey results or key findings.
#* Identify the audience to whom you will circulate the survey results or key survey findings.
#* Anticipate their reaction, including skepticism or difficulty comprehending the survey results, and factor that into your design, distribution, and analysis.
#* One way to generate prior buy-in is to circulate a survey draft among the key stakeholders, seek feedback from them, and incorporate it where it makes sense.
# Keep in mind the following heuristics for how your specific goals should color how you process the rest of this guide.
#* If you're mainly interested in qualitative feedback from survey respondents, some of the guidelines on statistical reliability are less important. Your main goal is just to get more feedback and get people to feel free to express them more, not to make sure that the set of responses you receive is representative of your entire audience.
#* If you are interested in conducting a single-question poll, you should focus on the design guidelines around a single question, and also on making sure your survey is distributed to your whole audience or a representative sample. You can ignore much of the other advice here.
#* If you intend to use the survey to further specific goals around convincing people, it is best to generate buy-in about the survey methodology and questions from them beforehand.

=== Understanding the Overall Steps ===

# Keep in mind the three main steps of using online surveys.
#* Survey design: This means creating the survey. It includes the selection of questions, selection of question order, selection of answer formats (e.g., free-form versus multiple-choice), and selection of design logic.<ref name=pew-questionnaire-design>[http://www.pewresearch.org/methodology/u-s-survey-research/questionnaire-design/ Questionnaire design], Pew Research Center</ref> It also includes the selection of the survey design tool.
#* Response collection: This means getting people to respond to the survey. When the total audience that you are trying to reach is small and responsive, this step is simple: you just ask them to fill in the survey. If, however, you want to get information about a large population that can't easily be reached, you'll need to find ways to properly ''sample'' from that large audience.
#* Data analysis: This involves looking at the survey responses and figuring out the answers to the questions that motivated you to make the survey in the first place.
# Keep in mind the dependencies between the selection of tools for these purposes.
#* In ''principle'', the steps are independent: you could use one tool for survey design, another tool for response collection, and a third tool for data analysis. In practice, there are limitations that they place on each other.
#* Some survey collection tools ''require'' the survey to be hosted on the associated survey design platform. For instance, if using Google Surveys for response collection, you have to host the survey on Google Surveys. Similarly, if using SurveyMonkey Audience for response collection, you have to host the survey on SurveyMonkey. On the other hand, you can use Amazon Mechanical Turk to get responses to surveys hosted anywhere that allows you to share a weblink: you just have to tell the Mechanical Turk participants to go to the survey link.
#* Conversely, some survey design tools only allow for response collection through the associated response collection tool. For instance, if you design a survey in Google Surveys, you can only direct responses to it through Google Surveys. On the other hand, surveys designed using SurveyMonkey can be distributed both through SurveyMonkey Audience and to your own selected audiences through the sharing of web links (with multiple collectors possible on a single survey).
#* Survey analysis can usually be done on the survey design platform itself. Many survey design platforms offer the option of exporting the survey results in formats including PDF, Excel, and the computer-friendly CSV format. You can process these exports through your own data analysis tools. However, the latter might require the expertise of a data scientist. Thus, much of your analysis of survey results is likely to be on the survey design platform itself.
# Keep in mind the interaction with your budget and goals. To use the right survey solution in a cost-effective way, you need to juggle a lot of different things. This guide will prepare you!
#* You need to have a good idea of the goals you wish to accomplish.
#* You need to have reasonable familiarity with survey design (something you will hopefully have after reading this guide). In cases where your target audience is too huge to survey completely, you'll also need the right statistical intuition and tools to interpret the results.
#* You need to know what your budget is. This becomes important when you are trying to reach reasonably representative samples of large target audiences.
#* You need a good mental model of the dependencies between the different components.

=== Selecting a Survey Design Tool ===

# Keep in mind the different functionalities offered by a survey design tool.
#* A survey design tool makes it easier for you to construct survey questions. Rather than write code to describe all the logic, you can select from pre-existing templates and designs. 
#* Some survey design tools also provide intelligent guidance on how to make your survey questions more statistically reliable.
#* The survey design tool also generally ''hosts'' the survey, and stores both the actual survey and all response information to the survey.
#* The survey design tool might have associated response collection tools (for instance, survey design tool SurveyMonkey has associated response collection tool SurveyMonkey Audience). Alternatively, it might simply allow you to generate a weblink that you can circulate to the desired survey audience. Some tools offer both.
#* The survey design tool provides some rudimentary online analysis of the survey results, and also allows you to download and export the responses. Typical download formats include comma-separated values (CSV), Microsoft Excel (XLS or XLSX), and Portable Document Format (PDF).
#* It is possible to imagine in principle that the actual question design part and the hosting part are completely separate: you can design a survey in one tool and then host it and collect responses on a different tool. However, in practice, most of the leading survey design tools combine both functions. Part of the reason is that online surveys involve reasonably complex web logic for which there is no global standard specification, and each tool differs somewhat in the capabilities it supports. So, you cannot upload a survey from one tool to another; you have to manually re-enter the questions.
# Keep in mind the following limitations placed by your survey design tool.
#* Some survey design tools, such as Google Forms, ''only'' allow web links, and don't directly integrate with response collection.<ref name=google-forms-response-collection>[https://support.google.com/docs/answer/2917686 Choose a form response destination], Google Support, September 24, 2016</reF> That means that response collection is ''your'' job: you'll need to take the web link and circulate it through your own distribution channels.
#* Survey design tools may not support features you care about: question types you want to use, branching and skip logic, ability to edit questions after the survey has started, ability to maintain multiple collectors for a single survey, integration with data analysis capabilities, unlimited responses, and data export capabilities. Some design tools (such as SurveyMonkey) offer some of these features in the free version, but restrict other, more advanced features to the paid version.<ref name=surveymonkey-pricing>[https://www.surveymonkey.com/pricing/upgrade/details/ SurveyMonkey Pricing]</ref>
# Use existing online comparisons of survey design tools. The Wikipedia page comparing survey software provides a starting point of a list.<ref name=wikipedia-comparison>[https://en.wikipedia.org/wiki/Comparison_of_survey_software Comparison of survey software], Wikipedia, the free encyclopedia, retrieved September 24, 2016</ref> WordStream also has a review of the best online survey tools, albeit this is from 2014 so some information may be outdated.<ref name=wordstream-survey-tool-comparison>[http://www.wordstream.com/blog/ws/2014/11/10/best-online-survey-tools 7 Best Survey Tools: Create Awesome Surveys For Free!], WordStream, November 10, 2014</ref> Some of the most common survey design tools, and their key advantages and limitations, are discussed below.
#* Google Forms
#** It is free and offers unlimited responses
#** it integrates with Google Sheets, where you can use spreadsheet commands to understand the responses. Responses can also be exported to Excel, CSV, or PDF for response analysis.<ref name=google-forms-response-collection/>
#** You can create reasonably complex surveys and question types.<ref name=google-forms-create-survey>[https://support.google.com/docs/answer/87809?hl=en&ref_topic=6063584 Create a survey using Google Forms], Google Support, retrieved September 24, 2016</ref> 
#** Upshot: It can be an ideal situation for a reasonably simple survey, and may be the best free tool available if you're getting over 1000 responses. 
#** One key ''disadvantage'' is a lack of integration with any response collection methods: you're basically left to your own devices for getting respondents.<ref name=google-forms-response-collection/>
#* SurveyMonkey 
#** Many pricing tiers starting with free (the BASIC plan).<ref name=surveymonkey-pricing/> 
#** BASIC plan good enough for most use cases: If your surveys have a linear flow (no skip logic or branching) and you don't expect over 100 responses per survey, the free version is good enough for you. 
#** Integration with SurveyMonkey Audience for response collection is a helpful feature (although SurveyMonkey Audience costs money every time you use it, it can be used with the free version of SurveyMonkey). 
#** You need to pay for advanced features such as: skip logic, over 100 responses on any survey, full data exports (beyond just PDF of a summary), multiple filters, and more. The cheapest plan is $26/month or $300/year. It is usually easy to switch down from paid to free, and you can continue to access past survey responses with over 100 responses.<ref name=surveymonkey-downgrading>[http://help.surveymonkey.com/articles/en_US/kb/Downgrading Downgrading to a BASIC Plan], SurveyMonkey, retrieved September 24, 2016</ref> 
#** One key ''disadvantage'' is that data export isn't available in the free version, though you can still make publicly shareable online links. But it means you are tied to the online platform unless you pay for at least one month, or manually copy over the data.<ref name=wordstream-survey-tool-comparison/> The other key disadvantage is the inability to view over 100 responses per survey, which can be an issue since you may need more than that many responses to get your entire target audience, or a large enough sample.<ref name=surveymonkey-sample-estimation>[http://help.surveymonkey.com/articles/en_US/SurveyMonkeyArticleType/How-many-respondents-do-I-need Calculating the Number of Respondents You Need], SurveyMonkey, retrieved September 24, 2016</ref><ref name=surveymonkey-significant-differences>[http://help.surveymonkey.com/articles/en_US/kb/Significant-Differences Statistical Significance], SurveyMonkey, retrieved September 24, 2016</ref>
#* Google Surveys
#** Google Surveys as a survey design tool is tied to Google Surveys as a response collection tool: to use one you have to use the other. Therefore, you should use this for design only if you are okay with using it to get responses. In particular, don't use this to survey you own friends or people in a small audience of yours.
#** It makes sense for short surveys, and is most ideal for single-question surveys.
#* Survata is a survey design tool with a similar model as Google Surveys: it is tied to its own distribution mechanism for the surveys. The advantages and disadvantages of Survata are discussed more in the section of choosing a response collection tool.<ref name=survata-v-gcs-dunrie>[http://scientificink.com/online-consumer-survey-comparison/ Online Consumer Survey Comparison: Survata vs. Google Consumer Surveys], Dunrie, Scientific Link, September 10, 2014 (note that some of the limitations and cost information are outdated)</ref><ref name=survata-v-gcs-mmr>[http://mmrstrategy.com/comparing-online-consumer-surveys-survata-vs-google-consumer-surveys Comparing Online Consumer Surveys: Survata vs Google Consumer Surveys], Dominique Romanowski, MMR Strategy Group, August 29, 2012 (note that some of the limitations and cost information are outdated)</ref>
#* PollDaddy is a survey tool made by the creators of WordPress, and is ideal for single-question poll surveys to embed on a website or blog post.<ref>[https://512citydesigner.wordpress.com/2013/06/17/surveymonkey-vs-polldaddy/ SurveyMonkey vs. Polldaddy], 512citydesigner, June 17, 2013</ref>
#* Facebook Polls is a survey design tool for creating surveys on Facebook. These can be circulated through Facebook the way Facebook posts usually circulate (with the circulation limited by the privacy settings of the poll and/or the group or page it was posted in). In addition, you can link to the poll from elsewhere to invite others to participate in it.<ref>[https://apps.facebook.com/my-polls Facebook Polls], retrieved September 24, 2016</ref><ref>[https://www.track5media.com/impromptu-market-research-google-consumer-surveys-and-facebook-polls/ Impromptu Market Research: Google Consumer Surveys and Facebook Polls], Ekom Ekyong, Track 5 Media, March 4, 2016</ref> Facebook Polls are ideal for single-question, multiple-choice surveys where your target audience is a defined audience that Facebook can reach. We discuss them more in the section on response collection tools.
#* Other survey solutions worth looking into include Qualtrics and SurveyGizmo.<ref>[http://usatoday30.usatoday.com/MONEY/usaedition/2012-08-28-Efficient-Small-Business-Ecommerce_CV_U.htm Customer research easier in digital era], Scott Martin, ''USA Today'', August 28, 2012</ref><ref name=wikipedia-comparison/>

=== Designing the Survey ===

# Keep in mind the following broad ideas.
#* There are two key concepts when evaluating how robust survey responses are: ''reliability'' and ''validity''. Reliable means that if the survey was administered again, you'd get similar responses. In other words, reliable means the survey is measuring ''something''. Valid means that what the survey measures is what you care about. For instance, if you ask people how good they are at knitting, the survey is reliable if you get the same distribution of responses whenever you administer the survey. It's valid if their answers are actually correct, i.e., how good they say they are at knitting matches how good they are at knitting.<ref name=survey-reliability-validity>[http://www.relevantinsights.com/validity-and-reliability/ Validity and Reliability in Surveys], Michaela Mora, Relevant Insights, February 21, 2011</ref>
#* People aren't perfectly rational and they aren't omniscient or omnipotent. Human limitations can affect the reliability and validity of surveys. However, not all is lost: by understanding the ways in which humans tend to mess up, you can improve your survey design. The systematic ways humans deviate from rationality are called cognitive biases.<ref name=cognitive-bias-codex>[https://betterhumans.coach.me/cognitive-bias-cheat-sheet-55a472476b18 Cognitive bias cheat sheet. Because thinking is hard.], Buster Benson, Better Humans, September 1, 2016</ref><ref>[http://lifehacker.com/this-graphic-explains-20-cognitive-biases-that-affect-y-1730901381 This Graphic Explains 20 Cognitive Biases That Affect Your Decision-Making], Patrick Allan, LifeHacker, September 16, 2015</ref>
#* Cognitive biases come in four clusters that arise from the following four phenomena: too much information, not enough meaning, need to act fast, and difficulty of remembering everything over the long term.<ref name=cognitive-bias-codex/> All of these influence how people respond in surveys. In the context of surveys, the need to act fast is the biggest factor, and too much information and not enough meaning can also play a role depending on the types of questions at hand.
#* The following are the specific biases most relevant to survey design:
#** The observer-expectancy effect, where survey respondents tailor their responses based on what they think the question-asker wants to hear. This is one of the reasons that question order matters.
#** Social desirability bias, where respondents try to answer in ways that make them look good.
#** Availability bias and related issues (such as the anchoring heuristic, bizarreness effect, base rate fallacy) where recently presented or other related information distorts people's answers.
#** Various forms of response bias, including acquiescence bias, moderation bias, and extreme response bias can lead people to, respectively, agree with you more than they mean to, choose moderate (middle-ground) answers, and choose extreme answers.<ref name=pew-questionnaire-design/>
# Keep in mind the following key points around things to watch out for as you design surveys.
#* The order of things matters. Randomization (at various levels) and A/B testing between different orders can help address this issue and determine the role of order.
#* The framing of questions matters. Some framings help combat biases (by cautioning people, or by addressing concerns such as those related to social desirability).
# Make sure each question makes sense to all your audience.
#* Think about whether your framing is inclusive to most of your target audience. In particular, make sure your questions make sense for respondents for all values of age groups, genders, occupation status, location, language, and any other criteria you care about.
#** An egregious example of forgetting this is, for instance, assuming that your survey respondent is male, but then circulating it to a mixed gender audience.
#** Subtler errors can arise in case that your implicit assumption holds for the majority of respondents you have in mind. For instance, a survey you target at United States college students might implicitly assume that the students live on campus or close to it, based on your own college experience. But many of your potential respondents might be commuter students.
#* In cases where your entire survey is aimed at only part of the audience to which it will inevitably be distributed, use disqualifying questions right at the beginning. This saves time for people who don't meet your criteria, as they can get disqualified early on. It also makes your analysis easier as you have fewer irrelevant responses contaminating your analysis.
#* In cases where some questions make sense only for a subset of the audience, but the survey as a whole is of interest to the entire audience, offer options of a "Not applicable" nature for the rest of your audience.
#* For more complex surveys, where subsets of the survey make sense only for subsets of the audience, use branch and skip logic.
#* Pilot tests and pretests can be used to identify issues related to your questions not making sense to your entire target audience. However, some issues may go unnoticed if the people you are pilot testing on do not include any of the anomalous cases.
# Frame your questions to tackle social desirability bias. Social desirability bias means that people say things that cast them in a good light. This is partly because they are fooling themselves.
#* Social desirability bias is both a characteristic of the individual survey respondent (some respondents are more prone to this bias than the others) and of the survey context.<ref name=wikipedia-sdb>[https://en.wikipedia.org/wiki/Social_desirability_bias Social desirability bias], Wikipedia, the free encyclopedia</ref> There is a Marlow-Crowne Social Desirability Bias Scale to judge the extent to which a given respondent has social desirability bias, and you can administer this as part of your survey.<ref name=ace-sdb>[http://www.animalcharityevaluators.org/research/foundational-research/survey-guidelines/social-desirability-scale/ Social Desirability Scale], Animal Charity Evaluators</ref><ref name=eaf-sdb>[http://effective-altruism.com/ea/105/using_amazons_mechanical_turk_for_animal_advocacy/ Using Amazon's Mechanical Turk for Animal Advocacy Studies: Opportunities and Challenges], Peter Hurford, Effective Altruism Form, August 2, 2016</ref> You can also reduce and control for social desirability bias through the framing of your survey questions.
#* Vegetarianism is one area where social desirability bias is strong: many people who claim to be vegetarian in surveys also report having eaten meat recently.<ref name=vegetarianism-pt>[https://www.psychologytoday.com/blog/animals-and-us/201109/why-are-there-so-few-vegetarians Why Are There So Few Vegetarians? Most "vegetarians" eat meat. Huh?], Hal Herzog, ''Psychology Today'', September 6, 2011</ref><ref name=vegetarianism-sdb-econlog>[http://econlog.econlib.org/archives/2013/07/vegetarianism_a.html Vegetarianism and Social Desirability Bias], Bryan Caplan, EconLog, July 16, 2013</ref> Another example is people reporting whether they vote.<ref>[http://web.stanford.edu/dept/communication/faculty/krosnick/Turnout%20Overreporting%20-%20ICT%20Only%20-%20Final.pdf Social Desirability Bias in Voter Turnout Reports: Tests Using the Item Count Technique], Allyson L. Holbrook and Jon A. Krosnick</ref> As the vegetarianism and voting examples indicate, social desirability bias can be exhibited even for cases where the behavior in question is not universally regarded as ideal. What matters is that there are enough people who view the behavior as desirable but may still end up not doing it. The Pew Research Center identifies the following other areas where social desirability bias is strong: "Research has shown that respondents understate alcohol and drug use, tax evasion and racial bias; they also may overstate church attendance, charitable contributions and the likelihood that they will vote in an election."<ref name=pew-questionnaire-design/> Wikipedia has a similar list.<ref name=wikipedia-sdb/>
#* Anonymity is a first step to combating social desirability bias. If your online survey asks for no personally identifiable information, that is usually enough to make respondents feel anonymous. However, there are cases where this is not sufficient to guarantee anonymity, usually when the audience is quite small so that it's possible to infer people's identity from their other responses. Some techniques, such as the randomized response technique and the unmatched count technique, have evolved to address this concern.<ref>[https://en.wikipedia.org/wiki/Randomized_response Randomized response], Wikipedia, the free encyclopedia</ref><ref>[https://en.wikipedia.org/wiki/Unmatched_count Unmatched count], Wikipedia, the free encyclopedia</ref> However, even a guarantee of anonymity may not be enough, because people might also be misleading themselves!
#* One way of combating social desirability bias is to make the question very specific. For instance, instead of asking whether a person is vegetarian, ask the person whether he or she ate meat in the last day. Instead of just saying "meat" specify a list of animal products. For instance, some self-identified vegetarians eat seafood (such as fish) since they consider that within the scope of vegetarianism. By making your question explicit on whether you include fish, you can help address any confusion.<ref name=vegetarianism-pt/><ref name=vegetarianism-sdb-econlog/> For more specifically on understanding people's dietary habits, look up more information on "food frequency questionnaire".<ref name=ace-sdb/>
#* A further step is to give people ready-made reasons so that they feel less bad about answering a question in a way that looks bad to them. For instance, the Pew Research Center's guideline on questionnaire design provides the following example of a way of gently asking people about whether they voted: "In the 2012 presidential election between Barack Obama and Mitt Romney, did things come up that kept you from voting, or did you happen to vote?"<ref name=pew-questionnaire-design/>
# For any individual question, choose carefully whether you'd like to use a closed-ended format (such as multiple-choice) or open-ended format. For closed-ended questions, select your options carefully.
#* The Pew Research Center's guide to questionnaire design notes that providing explicit choices makes it likely that people will select among those choices, even if a free response option is provided. This can be both an advantage and a disadvantage, depending on whether your goal is to obtain people's free thinking or get the items from a pre-specified list of options that come closest to their thinking.<ref name=pew-questionnaire-design/>
#* If using the closed-ended format, a useful tip for generating the options is to run a pilot test or focus group. Basically, ask a few people for free responses, then discuss it with them or let them discuss with each other. Then use the results of that discussion to formulate the options.<ref name=pew-questionnaire-design/> A similar approach has been used in the design of concept inventories, tests administered to help identify student misconceptions.(add citation)
#* One way of getting the benefits of a closed-ended format and an open-ended format is to first ask people for an open-ended response, and then present a list of options and ask which one best approximates their response. This can work best if they are not allowed to go back and edit their open-ended response.
#* If offering closed-ended options, consider randomizing the sort order. Randomization is a feature offered by most survey design tools. However, in cases that the options form a natural progression (for instance, from "use a lot" or "do not use at all"), keeping the sort order fixed helps. In such cases, randomizing between the original sort order and the ''reverse'' sort order, and then comparing the performance across the two sort orders, can help.<ref name=pew-questionnaire-design/> Google Surveys offers three options: full randomization, preserve the order of the questions, and randomize 50/50 between the order and its reverse (note, however, that when reporting results, they do not separately report results for the various sort orders used; they only report combined results). During their free review, they will suggest a more appropriate choice for you if what you selected does not match the guidelines presented here.
# Keep in mind the following when providing ranges for closed-ended options. For instance, suppose you want to ask people how often they use a product or service. To guide their thinking and make your analysis easier, you may offer this as a closed-ended question, with different buckets (example of buckets: fewer than once a week, 1 to 10 times a week, 11 to 25 times a week, 26 or more times a week). Your buckets partition the space of possibilities. When deciding the right buckets, keep the following in mind.
#* Make the buckets large enough that participants can provide an off-the-cuff answer with reasonable accuracy. At worst, they should hit one bucket too high or too low.
#* Make your buckets small enough that the analysis is meaningful.
#* Calibrate your selection of buckets to your audience! Often, people conducting surveys about a subject are steeped in thoughts of it, so overestimate usage, and select buckets that are on the high end. As a result most people end up going into the smallest bucket, leading to analysis that is not very meaningful. It is therefore useful to pretest to see what sort of buckets people select, and then dig deeper into those buckets.(add citation to Wikipedia usage survey results, quote on bucket estimation)
#* Another option is to use branching logic: offer coarse buckets, and then based on responses, ask for finer information for people at the extreme ends. If your primary interest is in users at or beyond a particular usage level, you can also use the usage level as a disqualifying question.
# Keep in mind the following types of response bias for specific questions. You can get a more full list of biases at the Wikipedia page on response bias.<ref name=wikipedia-response-bias>[https://en.wikipedia.org/wiki/Response_bias Respose bias], Wikipedia, the free encyclopedia</ref>
#* Acquiescence bias is a bias toward agreeing with the statement made in the question. This bias is strongest for questions with an "agree/disagree" or "yes/no" framing. This includes binary questions as well as Likert scales (such as the standard five-point scale of strongly disagree, disagree, neither agree nor disagree, agree, strongly agree). Like social desirability bias, acquiescence bias is partly a characteristic of the survey respondent: some responndents display it more than others. For instance, some research suggests that less educated respondents show more acquiescence bias.
#** One way to counteract is to reframe the question in a different way, for instance, by asking people to select the statement they agree with ''most'', rather than as a yes/no. 
#** Another approach (that helps quantify the extent of acquiescence bias, and therefore control for it) is to frame the question in terms of agreement/disagreement with one statement for (a randomly selected) half of the audience, and with the opposite statement for the other half. Alternatively two opposite statements can be placed in the same survey. Although redundant for a rational respondent, doing so can still be helpful to measure and control for acquiescence bias.
#* For rating scales, keep in mind the two kinds of bias: moderation bias (the bias toward selecting a middle response) and extreme response bias (the bias toward selecting an extreme response).  A typical example of a rating scale with a symmetric design is the Likert scale, with gradations from "strongly disagree" to "strongly agree".
#** There is no universal heuristic on the relative strength of moderation bias and extreme response bias. It can differ based on the type of question. In cases where people view an extreme response as more principled or confident, and value those qualities, extreme response bias can be stronger. In cases where they view moderate responses as more balanced and diplomatic, and value those qualities, moderation bias can be stronger.
#** To complicate things further, in the special cases of Likert scales (rating scales of agreement/disagreement) we also have to deal with acquiescence bias, discussed previously. So we are dealing with ''three'' sources of bias: acquiscence bias (that pushes people toward agreement), moderation bias (that pushes people to the middle), and extreme response bias (that pushes people toward the end).
#** One approach that might counteract moderation bias is the "forced-choice" method, namely, pick an ''even'' number of options in the scale. This prevents people from picking the exact midpoint, thereby revealing the direction they are inclined toward. There is considerable debate about the merits of the forced-choice method.
# Keep in mind that question order can affect your responses.
#* This is related to anchoring and the experimenter-observancy effect. The survey-taker uses previous questions to anchor his or her thinking, and is (consciously or subsconsciously) figuring out where you are trying to go. Another effect to keep in mind is the desire to appear consistent.
#* The Pew Research Center has found, in its questionnaire design, that when a general question follows a specific question, people tend to exclude the specific question from consideration in answering the general question. They call this a ''contrast effect'', and also note an effect of the general question on the specific question.
#* It may be possible to randomize the order of questions, or A/B test between different question orders.
#* If later questions make people reconsider earlier questions, it may be better to re-ask earlier questions ("in light of the new information, do you think ...") while ''not allowing'' people to go back and change the answers to earlier questions. This will give you a clearer sense of how people's thoughts on each question were affected by other questions.
#** See also the next point about survey length.
# Keep in mind how the length of the survey interacts with the quality of responses you get.
#* A lot depends on the context of administration. In general, however, respondents tend to lose interest after two minutes of filling online surveys, so in general it's a bad idea to design surveys where the median time to fill (ignoring any optional free response items) is more than two minutes. With that said, in cases where the survey is a high-stakes survey for the respondents, longer surveys may be fine.
#* In particular, for longer surveys, you may get fewer responses for the later questions, and the set of respondents you ''do'' get may be unrepresentative (in terms of having unusually high interest in completing the survey). 

=== Selecting a Response Collection Tool ===

# Keep in mind the following broad criteria when thinking of "paid audience" tools. A paid audience tool allows you to buy survey respondents at a fixed price per respondent. The price can vary based on the length of your survey as well as the targeting criteria and disqualifying or screening questions you use for respondents.
#* Cost, including cost per respondent and minimum spend requirements. Also, its interaction with the other features discussed below.
#* The overall geographical reach. SurveyMonkey Audience is active only in the United States, Australia, and the United Kingdom, with costs lowest in the United States. Google Surveys is active only in the United States, Canada, and the United Kingdom. Survata is actively only in the United States, Australia, Canada, and the United Kingdom. If you are interested in surveying people from a different country, you cannot use any of these tools.
#* The availability of targeting options, and how minimum cost and cost per respondent vary based on that. Targeting options and disqualifying questions allow you to have your survey responses come only from a segment of the population you care about.
#* The maximum number of questions you can have in the survey, and how minimum cost and cost per respondent vary based on number of questions.
#* The extent to which your survey respondents are a representative sample of your target audience. For instance, if you are trying to survey male Internet users aged 18 to 24 in the United States, how well do the users your survey reaches out to represent the population? Reliability is highest in cases where the audience explicitly enters the information (not necessarily on a per survey basis; some paid audience arrangements require respondents to enter their information once so that they can participate regularly). Others rely on anonymous web-tracking cookies and are less reliable on a per-user basis (though might still be reasonably reliable when averaged out across many users).
#* The seriousness with which your respondents fill in the survey. Note that seriousness of respondents can clash with representativeness: getting more serious respondents might entail sacrificing representativeness. There are other strategies to deal with non-serious respondents after the fact.
#* Your ability to use the same survey for other non-paid audiences. SurveyMonkey allows this, with multiple collectors on a single survey possible. Google Surveys does not.
#* Incentives used to attract your audience. This is mostly because of the effects on representativeness and seriousness. Note that paid audience tools differ in whether the participants themselves receive compensation. Mechanical Turk pays respondents based on the price you set, SurveyMonkey Audience donates to their charity of choice on their behalf, and Google Surveys and Survata use the "surveywall" model of allowing respondents to access paywalled content.
#* Your ability to edit the survey once it is live. SurveyMonkey offers this. Google Surveys and Survata do not, but they instead offer their own free human review before you launch the survey.
#* The speed of response: Most paid audience solutions offer to fill responses within a week. However, their speed in practice can differ widely.
# Keep in mind the following advantages and disadvantages of SurveyMonkey Audience.
#* Respondents come through SurveyMonkey Contribute. They are not monetarily compensated, but money is donated on their behalf to a charity of their choice.<ref>[http://help.surveymonkey.com/articles/en_US/kb/SurveyMonkey-Contribute SurveyMonkey Contribute], SurveyMonkey</ref> SurveyMonkey claims that this doesn't bias respondents to be more charitable, by comparing data on their respondents with Pew's averages for US and for the Internet.<ref>[https://www.surveymonkey.com/wp-content/uploads/2014/09/Audience-Charity-Bias-Whitepaper.pdf Charitable Bias Report], SurveyMonkey, September 2014</ref> Another thing to keep in mind is that SurveyMonkey Contribute encourages "survey addicts" by encouraging people to keep taking survey after survey while on the site.
#* To get a better sense of what it's like to be on the other side of the equation, you can sign up for SurveyMonkey Contribute and take a few surveys. You can generally take surveys in quick succession by clicking on the option to take a survey on the website.<ref>[https://github.com/vipulnaik/working-drafts/blob/master/surveymonkey-contribute.mediawiki SurveyMonkey Contribute], Vipul Naik, September 24, 2016</ref>
#* Advantage: It is part of SurveyMonkey, which offers many other survey tools and allows you to distribute surveys through weblinks. You can also circulate the same survey among different SurveyMonkey Audience audiences, and perform analysis of the results together.
#* Advantage: You can edit surveys ''after'' you have launched them with SurveyMonkey Audience, allowing you to fix any errors you discover based on the first few responses.
#* Advantage: You can browse through individual responses easily, as they come in in real time. This, combined with the ability to edit, means that you can iterate rapidly.
#* Advantage: Responses flow in quickly. If you buy 50 responses, you will usually get them in appproximately an hour.
#* Advantage: Demographic information used for targeting, as well as that displayed on a per-respondent basis, tends to be reasonably accurate, since it relies on respondents having entered the information in themselves when they signed up for SurveyMonkey Contribute. With that said, the people participating in SurveyMonkey Contribute may not be ''representative'' members of their demographics. In other words, if you see a respondent who is marked as female, then that respondent is very likely female, but the female respondents on SurveyMonkey Audience may not be representative of the general female population for your survey questions.
#* Mixed: Each SurveyMonkey Audience purchase is discrete. Rather than adding more responses to a given collector, you have to buy another batch of responses. This keeps your response collection clean but amplifies the below disadvantage.
#* Disadvantage: The cost of $1 per survey (and more if your survey is longer than 6 questions, and more for additional targeting) can make it expensive to reach statistical significance in the responses. Also, the minimum requirement of 50 responses makes it harder to test small numbers of respondents. Moreover, the fact that each purchase is discrete means that you cannot add on a few more responses to an already purchased survey: you always need to add at least 50.
#* Mixed: For each respondent, you can see demographic information (gender, age, location, etc.). However, not all the variables that can be used for targeting are visible this way. For instance, educational status can be used for targeting but is not available on a per-respondent basis for an untargeted survey.
#* Disadvantage: There is no free professional review of your survey. This is less of an issue because of the rapid feedback.
# Keep in mind the following advantages and disadvantages of Google Surveys.
#* Google Surveys is administered on the web as "surveywall" content. People need to answer the survey in order to access a specific piece of content on a website.<ref name=gcs-whitepaper>[https://www.google.com/insights/consumersurveys/static/consumer_surveys_whitepaper_v2.pdf Comparing Google Consumer Surveys to Existing Probability and Non-Probability Based Internet Surveys], Paul McDonald, Matt Mohebbi, Brett Slatkin, Google Inc.</ref> GCS has an online example to show what the user experience would look like.<ref>[https://www.google.com/insights/consumersurveys/publishers_example Publisher example], Google Surveys</ref>
#* Advantage: For single-question surveys, this can be cheap, at just 10 cents per response.
#* Advantage: You get free professional review of your survey, usually within 30 minutes of submitting the survey.
#* Advantage: The minimum spend per batch of responses is low. You can just spend $5 to get responses.
#* Advantage: You can use targeting and add screening questions. However, Google may ultimately reject your survey if it isn't able to get enough volume for your screening and targeting criteria.
#* Mixed: You cannot run multiple collectors on a single survey but you can incrementally add more responses to an existing survey. You can also run a survey on a weekly or monthly cadence to measure response trends.
#* Disadvantage: Costs can go up a lot once you get past a single question.
#* Disadvantage: For longer surveys, no single respondent sees more than two questions at a time. GCS cleverly stitches together responses to generate a coherent picture.
#* Disadvantage: You cannot edit the survey after it is submitted and active, nor can you cancel your payment. Professional review partly ameliorates the issue.
#* Disadvantage: Demographic information is inferred from the third-party DoubleClick cookie, so it is often inaccurate. For most users, this is not vetted by the user. However, users ''can'' edit the automatically inferred information and connect the information with the information they have entered in their Google accounts.<ref name=google-ad-settings>[https://www.google.com/settings/u/0/ads/authenticated?hl=en Ads Personalization], Google</ref>
#* Disadvantage: Responses are slower to arrive than with SurveyMonkey, taking 2 or 3 days to finish. The rate at which your survey is circulated depends on the total number of responses you buy. To get 50 responses in an hour, you may need to buy 1000 responses. Also, updates are done in batches: rather than seeing each response arrive in real time you will see updates every hour or so to your counts. Note that since you can't edit the survey anyway, this may not be a big deal.
# Keep in mind the following advantages and disadvantages of Survata.
#* Survata connects with publishers such as Doodle and HyperInk to find respondents. It limits the number of surveys a given respondent needs to fill, in order to tackle the problem of survey addicts. It puts in more effort to ensure representativeness and seriousness (for instance, by evaluating respondents through trap questions, and evaluating publisher representativeness by periodically running surveys with known frequencies of answers). But it also places more restrictions on the survey design.
#* Advantage: Survata offers free professional advice on survey design prior to launching the survey.
#* Mixed: The minimum cost per completed survey is $1. There is no equivalent of Google Surveys' 10 cent option.
#* Disadvantage: Survata limits surveys to six questions.
# If you are outside the United States, check for the available survey tools for your country. The main countries other than the United States for which standard survey tools offer options are Australia, Canada, and the United Kingdom. Moreover, costs per respondents are usually double of those in the United States (or more), and not all targeting options may be available.
#* For the United Kingdom, you can use SurveyMonkey Audience, Google Surveys, or Survata.
#* For Canada, you can use Google Surveys or Survata.
#* For Australia, you can use SurveyMonkey Audience or Survata.
#* Survata allowed you to target 17 countries in May 2015.
#* In India, two solutions you might be able to use are getinsights.co and pollfish.com.<ref>[https://www.quora.com/How-do-I-get-1000-quality-users-to-do-my-online-survey-in-India How do I get 1000 quality users to do my online survey in India?], Quora</ref>
#* IN general, consider alternatives to online surveys. Phone and in-person surveys may be more useful for reaching populations where the subset that has Internet access is small and unrepresentative. Standard polling firms (Nielsen, Gallup). In Africa, consider GeoPoll.


== References ==

<references/>

HuffPo: Customer Surveys: 5 Things You Need To Know, by Julia L. Rogers

HubSpot: How to Design a Marketing Survey that Yields Legitimate Results by Meghan Lockwood

Harvard Business School Working Knowledge: Using Surveys to Get the Information Your Business Needs - Survey Says? Identify Your Objectives

KissMetrics blog. Survey Questions That Work: How to Unlock Your Customers' Deepest Desires

- Informal surveys with a hidden advertising agenda (e.g. Facebook Polls)

- Repeat survey: construct questions that can be reused later so that trends can be studied

- Avoid narrow technical jargon (define if needed) and avoid hot-button words that are likely to trigger predetermined ideological stances (unless you are trying to elicit those predetermined stances)

- Provide full range

- Distribution issue: email spam
