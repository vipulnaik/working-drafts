This how-to guide goes over general principles for troubleshooting regressions in production systems controlled by some mix of software (code), configuration, and hardware. The relative importance of these general principles can vary widely between systems, based on several factors such as the domain in which the systems operate and how the systems are maintained.

The guide is specifically focused on "regressions" -- things that used to work and seem to no longer be working. Some, but not all, of the principles may also apply to broader troubleshooting of ongoing anomalies.

Principles from this guide can also be incorporated into the design of troubleshooting guides specific to particular systems, in addition to guidance that is ''specific'' to those systems.

== Steps ==
=== Pinpointing the Regression Point in the Changelog ===
# If a changelog is available, locate it: the changelog should start at a time prior to the regression and end at a time after the regression was introduced. Ideally, the changelog should includee release times as well as change set descriptions and should allow for the viewing of all details a specific released version and the differences (diffs) between any two versions.
#* In cases where changes are made through code, the full list of changes may be available as a commit history in the version control tool, such as git, and accessible in a corresponding online interface, such as GitHub. However, not every commit may have been released, and the time of release may not match the time of commit. Release tags may be used to demarcate what commits were released.
#* Changes made to configurations through a custom UI may be viewable in the form of an edit history in that UI, or in an associated database.
#* If changes are pushed out through releases, a changelog file may be available that lists releases along with dates as well as the set of changes going out with each release.
# Where possible and necessary, filter the set of releases to releases that could plausibly have introduced the regression.
#* For instance, for releases of software code, simple analysis of the code and the nature of the regression may imply that only releases that changed specific files could have caused the regression. Filtering to only those releases may reduce the number of releases we need to examine. (Version control tools such as git offer handy functionality for such filtering).
# Use a suitable search method (such as binary search aka bisection, or linear search) to find exactly where the regression was introduced.
#* This works best if it is possible to inspect any version to see if it has the regression. Such inspection could be by running a simulated environment that runs that version, or it could be by examining the changes from the previous version to that version, or by examining historical metrics around the time of that version's release (see Method 2).
#* Binary search, or bisection, works best in cases where each version can be fully simulated to test whether the regression is present in it (without necessarily knowing if this was the ''first'' version with the regression), and the cost of simulating any version is about the same (and the order in which we simulate doesn't affect the cost). Here, at each stage, we inspect a version that is midway between the newest version that we know to not have the regression and the oldest version that we know to have the regression. Binary search can find the point at which the regression was introduced in about as many iterations as the binary logarithm of the number of releases we are inspecting.
#* Linear search (going through the versions in linear order) may work better in cases where the incremental cost of simulating a version is lower after simulating an adjacent version.
#* In some cases, probabilistic information about where and when the regression most likely happened can allow us to do better than binary search.
# When releases bundle together multiple commits, you may want to first identify the release that introduced the regression, then look within that to identify the commit.
#* Identifying the release first may be faster as there are fewer releases than commits, and releases are more likely to be coherent units to test (individual commits may, for instance, have fatal bugs that were fixed in later commits prior to release). Another advantage of dealing with releases is that they tie more closely to production metrics, that we examine in Method 2. The process of identifying the release may therefore require less subject matter knowledge and could be done by somebody lacking the background to study individual commits.
#* Once a release is identified, and particularly if it fits well with Method 2, the analysis of individual commits within the release has to be done without the aid of production metrics, and may require more skill in terms of being able to simulate environments and examine diffs.
# If historical examination and simulation narrow down the likely causes but don't pinpoint the exact cause, consider redeploying to production with some of the causes reverted, to get more information about what the real cause is.
#* For instance, if we pinpoint the release but it comprises two changes A and B, and we're not sure which of the changes caused the regression, we can try deploying to production with A reverted. If reverting A fixes the regression in production, A is the likely cause. If it didn't, B is the likely cause.
#* When the number of candidates is more than two, we can adopt a binary search, or divide-and-conquer, strategy of reverting half the changes and seeing if the regression is fixed, then repeating. However, since these reverts affect production, we may want to be more conservative and try reverting changes one at a time (a linear search).
#* We may have to take into account the likely interactions between the candidate changes, as well as their interactions with other changes that have happened since then. Those other interacting changes may make the candidate changes hard to revert.

=== Using Production Metrics to Find When the Regression Happened ===

# Identify a good metric or set of metrics that can be used to detect the regression, and try to reduce as much as possible to a metric expected to have a clear before/after difference.
#* The most ideal case is a metric that is zero (or near-zero) prior to the regression and becomes (clearly) non-zero after the regression. For instance, if the regression in question is "occasional HTTP 500 errors" then the number of HTTP 500 errors may be an appropriate metric. The opposite case is also ideal: a metric that is non-zero prior to the regression and zero (or near-zero) after the regression.
#* In some cases, we can't find quite such a metric, but instead, can find a metric that sharply increases, or sharply decreases, when the regression is introduced, compared to the level of fluctuation seen before and/or after the introduction of the regression (mathematically, this is similar to the previous case applied to the derivative of the metric).
#* Sometimes, we are looking at how the relationship between two metrics changes with the introduction of the regression. For such cases, it is usually helpful to try to reduce to the preceding case by taking either the difference or the ratio of the metrics.
#* In general, metrics that react more quickly (with a direct causal relationship and with minimal time lag) are better than metrics that react with a large or variable time lag.
# Think about the expected time lag between the release of the regression and it showing up in the metric(s), and the ''shape'' of the rollout. This will inform the sort of patterns we are looking for in the graph of the metric(s).
#* For instance, a software update for software used in a production system may take a few hours to roll out to all parts of the production system (for instance, if a rolling update is used to replace all the servers). During this rollout process, an increasing proportion of the production system suffers from the regression. The metric in question may therefore transition gradually over the rollout period from the pre-regression value to the post-regression value. If the rollout has constant speed, the transition should be linear.
#* A release that affects a website with caching may take time to affect users still getting the cached older version. Over the cache time-to-live (TTL) users gradually move from the older cached version to the new version; the metric may also transition gradually over this period. The shape of the transition depends on the cache hit ratio; as a general rule, the transition is fastest at the beginning and then slows down.
#* A release that requires end users to proactively update would roll out as end users get around to updating; the speed of this rollout could be affected by a wide variety of factors. For instance, in some cases, updates are scheduled at night, so the bulk of the rollout may happen at night in the users' timezone.
#* In some cases, a release has some indirect downstream effect, through multiple steps of feedback, on the metric. For instance, a regression that breaks some of the page styling may make the page look more ugly, causing users to stop visiting the site over time and causing search engines to stop recommending it. Such indirect downstream effects may take time to manifest: what's really important in such cases is to have as clear a sense as possible of the lags involved, including the lag from the change to the ''start'' of its effects.
# Choose a suitable granularity and moving window of the time series chart of the metric(s).
#* There are two related concepts: the "moving window" for each data point, which represents the duration of time over which that data is aggregated, and the "granularity" of the chart, that represents the time difference between adjacent time points in the chart. For many charting tools, the granularity and moving window are the same, but some charting tools allow for a larger moving window than the granularity (e.g., each data point could use a moving window of 1 week of data, and we could have data points at daily granularity, so adjacent data points are using overlapping data). Where possible, use a charting tool that allows for separate selection of moving window and granularity.
#* Choose a granularity that is small enough (fine enough) to be able to identify the shape of the transition in the metric during the rollout.
#* Choose a moving window that is large enough (coarse enough) to suppress moment-to-moment noise, as well as time cycles that are at smaller granularity than the rollout.
#* When the granularity and moving window are required to be equal, choose a value for both that balances the above two considerations.
#* If the granularity you desire is finer than what you have available (for instance, if the finest available granularity is several times larger than the time the change takes to roll out), choose the finest available granularity, then use estimation techniques for finer estimation of the time (see next point).
# Scan the time series chart of the metric(s) at the desired granularity to identify when the regression happened.
#* Look for the ''start'' of change of the pattern in metric from the pre-regression pattern to the post-regression pattern. That should roughly match the release time (except in cases where there is a lag even to the ''start'' of the metric changing). In case of moving windows, pay attention to conventions around whether the time axis label is the beginning, middle, or end of the moving window.
#* Check if the ''shape'' of the change from the pre-regression pattern to the post-regression pattern matches the shape expected based on the mechanism of lag. This is particularly useful if the shapes are different for different candidates for what caused the regression. In such cases, you may be able to use shape to rule some candidates out. In general, though, shape should be a secondary consideration to start time.
#* When the granularity isn't as fine as you want, use estimation techniques to get finer. For instance, if data is only available at a daily granularity for a change that rolls out instantaneously, and the value pre-regression is 0 and the value post-regression is 1, then by using the value of the metric on the day of the transition, we can get an estimate of how far in the day the transition occurred. For instance, if it's at 0.5, that suggests that the transition happened around the middle of the day (traffic-wise, so not necessarily the middle time-wise).
#* In some cases, there's only one release that matches the start time. In other cases, due to the granularity not being fine enough, or due to other issues such as measurement error and random noise, you only get a time range rather than an exact time, and multiple releases within that time frame are candidates. The previous method of using the changelog can then be applied for this subset of releases.

=== Using Filtering and Segmentation ===

# Use multiple filters/labels/segments, and use the breakdown across those to get a clearer picture.
#* Examples of filters/labels/segments in a web traffic context include device type (desktop, mobile, tablet), operating system, browser, country, whether the user is a new or repeat visitor, and connection speed. For instance, if we use device (desktop, mobile, tablet) and connection speed (2G, 3G, and 4G), we can combine them to form 3 X 3 = 9 segments (desktop 2G, desktop 3G, desktop 4G, mobile 2G, mobile 3G, mobile 4G, tablet 2G, tablet 3G, tablet 4G).
#* Different causal explanations may make different predictions about what filters/labels/segments will be affected and in what direction.
#* If time series data is available for various segments, look for when all the segments (that we expect to affect) simultaneously move in the expected direction.
# Combine trend data across segments to pinpoint the time of the regression.
#* As a general rule, if the hypothesis strongly predicts that all segments should move together (i.e., at around the same time), it's more important to find the time when all or most segments move in the same direction, than to look for the largest moves for any individual segment (such an individual move might be a result of noise or a result of factors specific to that segment).
#* Charting all the segments in the same time series chart can often provide the clearest visual confirmation of when they all started changing. However, this can be challenging when the number of segments is large and when each of them is noisy. There can also be issues around the metrics for different segments not all being of the same order of magnitude (some charting tools don't support different scales for different metrics in the same chart).
#* An alternative is to plot, over time, the number of segments that move up or down compared to the preceding time interval. For instance, at a daily granularity, we may measure, on each day, the number of segments where the metric has decreased relative to the previous day. If we plot this number over time, we can identify times when a large proportion of segments decrease together.
#* A related method is to take differences or ratios of metric values over successive days, then plot the range of these ratios as a function of time. For instance, let's say we have segments based on device type (desktop, mobile, tablet) and are using a daily granularity. For each day, we'd have three ratios: desktop ratio of that day over the previous day, mobile ratio of that day over the previous day, tablet ratio of that day over the previous day. We plot these three ratios over time. We can then identify times when the largest of the three ratios is less than 1 and as small as possible (which means that they're all going down the most) or times when the smallest of the three ratios is greater than 1 and as large as possible.
# Consider adding labels for the version of code or configuration being used, so that production metrics can be filtered and segmented by code version.
#* Normally, different versions of code and configuration apply at different times, so in principle a time series chart should suffice and having the version of code or configuration as a segment should be superfluous.
#* However, if there is a lag between the release of a new version and its full rollout, the time period of rollout may have a traffic split between the two versions. Segmenting by version allows us to get clearer insight into the behavior during that transition period, specifically whether the issue is with the version update, or with something else that happened at around the same time.
#* When the production system is a patchwork of several pieces of code and configuration, each subject to their own versioning, and each released in a different way, it's particularly helpful to automate the process of labeling versions for each of the pieces of code and configuration in the production metrics. This allows for easier identification across all the pieces of code or configuration. The alternative method of looking at the changelog for all the pieces of code and configuration could be time-intensive and may be infeasible for somebody without subject matter expertise in all areas.

=== Debugging Regressions with Delayed Impact ===

# Use this cluster of methods in cases where the regression's impact is not immediate or with a fixed, predictable lag, and cannot be easily verified by simulation or emulation of past versions.
#* The underlying reason for this is that the regression is only one of several pieces that need to come together to cause issues. For instance, a data center may have some code and configuration to handle electrical power failures. If these electrical power failures are relatively rare, regression in the code may not be caught until the next hardware failure, and there may be several version updates in the interim.
#* There are two flavors of this: one where the other factors needed to cause the issue are relatively rare and happen from time to time. The other is where there's some kind of state change in the other factors that causes them to happen much more frequently. For instance, perhaps wildfire season results in many more power outages.
# Use historical data, including data from other similar environments, to determine if there was any regression at all.
#* It's possible that there was no regression, and that the system we're in charge of has always had this weakness, and this weakness is now having more of an impact because of the changes to other factors (e.g., wildfire season is much more severe).
#* To evaluate that possibility, look at data over a long enough time period, or a large enough range of environments, so that there are more examples of cases where the other factors were more common (e.g., past wildfire seasons that were comparably severe, and led to similar power outages). If the lifetime of the systems in question is long enough, we may be able to find such examples.
#* In the absence of clear data to indicate a regression, it may be best to troubleshoot the issue just as a bug and not assume it to be a regression. Further investigation may still reveal it to be a regression.
# If a clear past period is found where the system seemed more robust to external factors being bad, look for the regression between then and the time it was known to be failing.
#* This provides a time range; it should now be possible to examine releases within that time range using the previously discussed metrics.
#* If this method is not yielding results, it may be best to troubleshoot the issue just as a bug and not assume it to be a regression.

__METHODS__
