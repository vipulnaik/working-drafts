This how-to guide goes over general principles for troubleshooting regressions in production systems controlled by some mix of software (code), configuration, and hardware. The relative importance of these general principles can vary widely between systems, based on several factors such as the domain in which the systems operate and how the systems are maintained.

The guide is specifically focused on "regressions" -- things that used to work and seem to no longer be working. Some, but not all, of the principles may also apply to broader troubleshooting of ongoing anomalies.

Principles from this guide can also be incorporated into the design of troubleshooting guides specific to particular systems, in addition to guidance that is ''specific'' to those systems.

== Steps ==
=== Pinpointing the Regression Point in the Changelog ===
# If a changelog is available, locate it: the changelog should start at a time prior to the regression and end at a time after the regression was introduced. Ideally, the changelog should includee release times as well as change set descriptions and should allow for the viewing of all details a specific released version and the differences (diffs) between any two versions.
#* In cases where changes are made through code, the full list of changes may be available as a commit history in the version control tool, such as git, and accessible in a corresponding online interface, such as GitHub. However, not every commit may have been released.
#* Changes made to configurations through a custom UI may be viewable in the form of an edit history in that UI, or in an associated database.
#* If changes are pushed out through releases, a changelog file may be available that lists releases along with dates as well as the set of changes going out with each release.
# Where possible and necessary, filter the set of releases to releases that could plausibly have introduced the regression.
#* For instance, for releases of software code, simple analysis of the code and the nature of the regression may imply that only releases that changed specific files could have caused the regression. Filtering to only those releases may reduce the number of releases we need to examine. (Version control tools such as git offer handy functionality for such filtering).
# Use a suitable search method (such as binary search aka bisection, or linear search) to find exactly where the regression was introduced.
#* This works best if it is possible to inspect any version to see if it has the regression. Such inspection could be by running a simulated environment that runs that version, or it could be by examining the changes from the previous version to that version, or by examining historical metrics around the time of that version's release (see Method 2).
#* Binary search, or bisection, works best in cases where each version can be fully simulated to test whether the regression is present in it (without necessarily knowing if this was the ''first'' version with the regression), and the cost of simulating any version is about the same (and the order in which we simulate doesn't affect the cost). Here, at each stage, we inspect a version that is midway between the newest version that we know to not have the regression and the oldest version that we know to have the regression. Binary search can find the point at which the regression was introduced in about as many iterations as the binary logarithm of the number of releases we are inspecting.
#* Linear search (going through the versions in linear order) may work better in cases where the incremental cost of simulating a version is lower after simulating an adjacent version.
#* In some cases, probabilistic information about where and when the regression most likely happened can allow us to do better than binary search.
# (Something about commit-level granularity versus release-level granularity, and reverting an individual commit versus a release -- more granularity makes diffs easier to reason about, but release-level ties better to the review of production metrics)

=== Using Production Metrics to Find When the Regression Happened ===

# Identify a good metric or set of metrics that can be used to detect the regression, and try to reduce as much as possible to a metric expected to have a clear before/after difference.
#* The most ideal case is a metric that is zero (or near-zero) prior to the regression and becomes (clearly) non-zero after the regression. For instance, if the regression in question is "occasional HTTP 500 errors" then the number of HTTP 500 errors may be an appropriate metric. The opposite case is also ideal: a metric that is non-zero prior to the regression and zero (or near-zero) after the regression.
#* In some cases, we can't find quite such a metric, but instead, can find a metric that sharply increases, or sharply decreases, when the regression is introduced, compared to the level of fluctuation seen before and/or after the introduction of the regression (mathematically, this is similar to the previous case applied to the derivative of the metric).
#* Sometimes, we are looking at how the relationship between two metrics changes with the introduction of the regression. For such cases, it is usually helpful to try to reduce to the precedig case by taking either the difference or the ratio of the metrics.
# Think about the expected time lag between the release of the regression and it showing up in the metric(s), and the ''shape'' of the rollout. This will inform the sort of patterns we are looking for in the graph of the metric(s).
#* For instance, a software update for software used in a production system may take a few hours to roll out to all parts of the production system (for instance, if a rolling update is used to replace all the servers). During this rollout process, an increasing proportion of the production system suffers from the regression. The metric in question may therefore transition gradually over the rollout period from the pre-regression value to the post-regression value. If the rollout has constant speed, the transition should be linear.
#* A release that affects a website with caching may take time to affect users still getting the cached older version. Over the cache time-to-live (TTL) users gradually move from the older cached version to the new version; the metric may also transition gradually over this period. The shape of the transition depends on the cache hit ratio; as a general rule, the transition is fastest at the beginning and then slows down.
#* A release that requires end users to proactively update would roll out as end users get around to updating; the speed of this rollout could be affected by a wide variety of factors. For instance, in some cases, updates are scheduled at night, so the bulk of the rollout may happen at night in the users' timezone.
# Choose a suitable granularity and moving window of the time series chart of the metric(s).
#* There are two related concepts: the "moving window" for each data point, which represents the duration of time over which that data is aggregated, and the "granularity" of the chart, that represents the time difference between adjacent time points in the chart. For many charting tools, the granularity and moving window are the same, but some charting tools allow for a larger moving window than the granularity (e.g., each data point could use a moving window of 1 week of data, and we could have data points at daily granularity, so adjacent data points are using overlapping data). Where possible, use a charting tool that allows for separate selection of moving window and granularity.
#* Choose a granularity that is small enough (fine enough) to be able to identify the shape of the transition in the metric during the rollout.
#* Choose a moving window that is large enough (coarse enough) to suppress moment-to-moment noise, as well as time cycles that are at smaller granularity than the rollout.
#* When the granularity and moving window are required to be equal, choose a value for both that balances the above two considerations.
#* If the granularity you desire is finer than what you have available (for instance, if the finest available granularity is several times larger than the time the change takes to roll out), choose the finest available granularity, then use estimation techniques for finer estimation of the time (see next point).
# Scan the time series chart of the metric(s) at the desired granularity to identify when the regression happened.
#* Look for the ''start'' of change of the pattern in metric from the pre-regression pattern to the post-regression pattern. That should roughly match the release time. In case of moving windows, pay attention to conventions around whether the time axis label is the beginning, middle, or end of the moving window.
#* Check if the ''shape'' of the change from the pre-regression pattern to the post-regression pattern matches the shape expected based on the mechanism of lag.
#* When the granularity isn't as fine as you want, you estimation techniques to get finer. For instance, if data is only available at a daily granularity for a change that rolls out instantaneously, and the value pre-regression is 0 and the value post-regression is 1, then by using the value of the metric on the day of the transition, we can get an estimate of how far in the day the transition occurred. For instance, if it's at 0.5, that suggests that the transition happened around the middle of the day (traffic-wise, so not necessarily the middle time-wise).
#* In some cases, there's only one release that matches the start time. In other cases, due to the granularity not being fine enough, or due to other issues such as measurement error and random noise,, you only get a time range rather than an exact time, and multiple releases within that time frame are candidates. The previous method of using the changelog can then be applied for this subset of releases.
